{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instacart Market Basket Analysis: Data Wrangling, Exploratory Data Analysis,Data Visualization, Algorithms and Machine Learning\n",
    "\n",
    "\n",
    "The overall goal of this iPython notebook serves as the code for the final project - Instacart Market Basket Analysis. Data Cleaning has been done to get the data in a format needed for further analysis. Data manipulation, and data exploration has been implemented to uncover interesting insights from the data. Data tranformation, and merging different datasets for analysis have been carried out as well. Inferential statistics have also been performed.Data visualizations have been used to tell a story with the data and get a better understanding of Instacart users.Finally, the notebook consists of the models and algorithms that were implemented. It also contains feature engineering that were performed for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisles= pd.read_csv('./aisles.csv')\n",
    "departments=pd.read_csv('./departments.csv')\n",
    "order_products_prior=pd.read_csv('./order_products__prior.csv')\n",
    "order_products_train=pd.read_csv('./order_products__train.csv')\n",
    "orders=pd.read_csv('./orders.csv')\n",
    "products=pd.read_csv('./products.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products_prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_products_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll clean the data and look at the missing values in each data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for missing values\n",
    "total=orders.isnull().sum()\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for the percentage\n",
    "percentage=total/orders.isnull().count()\n",
    "percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_table_orders = pd.concat([total,percentage],keys=['Total','Percentage'],axis=1)\n",
    "missing_value_table_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that only 6% of days_since_prior_order column is null. So we can exclude them and use the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_new=orders[orders['days_since_prior_order'].notnull()]\n",
    "orders_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we check for missing values for all the other 5 data sets to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aisles\n",
    "total_a=aisles.isnull().count()\n",
    "total_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_a=total_a/aisles.isnull().count()\n",
    "percentage_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_table_aisles = pd.concat([total_a, percentage_a],keys=['Total','Percentage'],axis=1)\n",
    "missing_value_table_aisles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#departments\n",
    "total_d=departments.isnull().count()\n",
    "total_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_d=total_d/departments.isnull().count()\n",
    "percentage_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_table_departments = pd.concat([total_d,percentage_d],keys=['Total','Percentage'],axis=1)\n",
    "missing_value_table_departments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#orders_prior\n",
    "total_order_p_p=order_products_prior.isnull().sum()\n",
    "total_order_p_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_order_p_p=total_order_p_p/order_products_prior.isnull().count()\n",
    "percentage_order_p_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_table_order_p_p = pd.concat([total_order_p_p,percentage_order_p_p],keys=['Total','Percentage'],axis=1)\n",
    "missing_value_table_order_p_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order_train\n",
    "total_order_train=order_products_train.isnull().sum()\n",
    "total_order_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_order_train=total_order_train/order_products_train.isnull().count()\n",
    "percentage_order_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_table_order_train = pd.concat([total_order_train,percentage_order_train],keys=['Total','Percentage'],axis=1)\n",
    "missing_value_table_order_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#products\n",
    "total_products=products.isnull().sum()\n",
    "total_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_products=total_products/products.isnull().count()\n",
    "percentage_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value_table_products = pd.concat([total_products,percentage_products],keys=['Total','Percentage'],axis=1)\n",
    "missing_value_table_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the other 5 data sets we see that there are no missing values and hence conclude the data cleaning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis & Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try to get the count of the three evaluation set prior,train and test and then plot them to get an idea about the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=orders['eval_set'].value_counts()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(count.index, count.values)\n",
    "plt.ylabel('Number of Occurrences in the dataset', fontsize=14)\n",
    "plt.xlabel('Evaluation set type', fontsize=14)\n",
    "plt.title('Eval_set breakdown in orders dataset', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph we can see that that test evaluation as 75000 samples on which we will obtain predictions. Now let's see the distribution with respect to the hour of the day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_hour_of_day=orders['order_hour_of_day'].value_counts()\n",
    "count_hour_of_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.countplot(x=\"order_hour_of_day\", data=orders,palette='BuGn_d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above we can see that the maximum number of orders is around 10 and 11AM followed by 3-4PM which makes sense since morning is the time before lunch and dinner. On the other hand the orders are least at 3-4AM since that's the time people are asleep. Now, in order to know more. let's see how the orders vary across different days of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dow=orders['order_dow'].value_counts()\n",
    "count_dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax = sns.countplot(x=\"order_dow\", data=orders,palette='GnBu_d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph we can see that, the number of orders is maximum on Sunday and Monday which makes sense since people want to shop for groceries either at the start of the week or in the weekend. On the other hand, it's least in the middle of the week which is thursday followed by wednesday. Now to know more, let's look at the orders with respect to the hours on a given day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax=sns.countplot(x='order_hour_of_day',data=orders[orders['order_dow']==0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the plot of orders on Sunday. We can see that maximum number of orders are placed around 2-3pm. similarly, let's look at the orders by hour distribution for the rest of the days to get an idea about weekend vs weekdays patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax=sns.countplot(x='order_hour_of_day',data=orders[orders['order_dow']==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So on monday, the peak is reached at 10AM. So most of the orders are placed in the morning on Monday from 9-11AM. Followed by 12-3pm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "ax=sns.countplot(x='order_hour_of_day',data=orders[orders['order_dow']==2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuesday also pretty much follows the same trend as Monday. Having most orders in the morning from 10-11AM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wednesday\n",
    "plt.figure(figsize=(12,8))\n",
    "ax=sns.countplot(x='order_hour_of_day',data=orders[orders['order_dow']==3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#thursday\n",
    "plt.figure(figsize=(12,8))\n",
    "ax=sns.countplot(x='order_hour_of_day',data=orders[orders['order_dow']==4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#friday\n",
    "plt.figure(figsize=(12,8))\n",
    "ax=sns.countplot(x='order_hour_of_day',data=orders[orders['order_dow']==5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saturday\n",
    "plt.figure(figsize=(12,8))\n",
    "ax=sns.countplot(x='order_hour_of_day',data=orders[orders['order_dow']==6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphs for wednesday,Thursday and Friday pretty much follow the trend for Monday and Tuesday. But in the above graph for Saturday, the peak times is in the afternoon around 2-3pm. So we can see that, during the weekends, peak orders are in the afternoon from 2-4pm whereas in the weekdays, it's in the morning from 10AM-12PM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get the orders in terms of hour of the day and day of the week in a single dataset by using the groupby option for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_orders = orders.groupby([\"order_dow\", \"order_hour_of_day\"])[\"order_number\"].aggregate(\"count\").reset_index()\n",
    "grouped_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivoting the table for clarity\n",
    "grouped_orders = grouped_orders.pivot('order_dow', 'order_hour_of_day', 'order_number')\n",
    "grouped_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "sns.heatmap(grouped_orders)\n",
    "plt.title(\"Frequency of Day of week Vs Hour of day\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above heatmap, we can see that peak orders are in the afternoon on Sunday and Monday, from 9AM-4PM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.countplot(x=\"days_since_prior_order\", data=orders)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "plt.xlabel('Days since prior order', fontsize=14)\n",
    "plt.title(\"Frequency distribution by days since prior order\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this plot we can see that 7th day is where we have a spike, and then a relative small peak at days 14,21 and 28 which indicates that every 7 days or weekly is the order frequency. And then again there's a huge peak at the end of the month indicating that there's a monthly peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of re-orders in orders_products_prior\n",
    "order_products_prior.reordered.sum() / len(order_products_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 59% of the products are re-ordered from the prior dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of re-orders in orders_products_train\n",
    "order_products_train.reordered.sum() / len(order_products_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 60% of the products are re-ordered from the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging order_products_prior and products\n",
    "op_prior_merged = pd.merge(order_products_prior, products, on='product_id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging op_merged with aisles\n",
    "op_prior_merged = pd.merge(op_prior_merged, aisles, on='aisle_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the new op_prior_merged with departments\n",
    "op_prior_merged= pd.merge(op_prior_merged, departments, on='department_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's see the new op_merged\n",
    "op_prior_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#del op_prior_merged['department_y']\n",
    "#op_prior_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_products = op_prior_merged['product_name'].value_counts().reset_index().head(20)\n",
    "count_products.columns=['product_name','frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "sns.barplot(count_products.product_name, count_products.frequency, alpha=0.8)\n",
    "plt.ylabel('Frequencies', fontsize=14)\n",
    "plt.xlabel('Products', fontsize=10)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, the product that is most ordered are fruits like bananas, strawberries and organic products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_aisles = op_prior_merged['aisle'].value_counts().head(20)\n",
    "count_aisles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "sns.barplot(count_aisles.index, count_aisles.values, alpha=0.8)\n",
    "plt.ylabel('Frequencies', fontsize=14)\n",
    "plt.xlabel('Aisle', fontsize=10)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph we can see that the fresh food and fresh vegetables aisles are the most frequently visited. We can do the same analysis for department and also check the reordered items against day of the week and the hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_dept = op_prior_merged['department'].value_counts()\n",
    "count_dept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,15))\n",
    "sns.barplot(count_dept.index, count_dept.values, alpha=0.8)\n",
    "plt.ylabel('Frequencies', fontsize=14)\n",
    "plt.xlabel('Departments', fontsize=10)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph we can see that the department wise frequency is more for produce which aligns with the aisles frequency and then for dairy eggs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge order_product_prior with orders \n",
    "merged_reorders = pd.merge(order_products_prior, orders, on='order_id', how='left')\n",
    "merged_reorders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_reordered = merged_reorders['reordered'].value_counts()\n",
    "count_reordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,12))\n",
    "sns.barplot(count_reordered.index, count_reordered.values)\n",
    "plt.ylabel('Frequencies', fontsize=14)\n",
    "plt.xlabel('Reordered', fontsize=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding reorders against day of the week\n",
    "grouped_reorders_dow = merged_reorders.groupby([\"order_dow\"])[\"reordered\"].aggregate(\"count\").reset_index()\n",
    "grouped_reorders_dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,12))\n",
    "sns.barplot(grouped_reorders_dow.order_dow, grouped_reorders_dow.reordered)\n",
    "plt.ylabel('Total number of reordered products', fontsize=14)\n",
    "plt.xlabel('order_day_of_week', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, we can see that most products are reordered on Sunday followed by Monday and Saturday. Which follows the same trend as orders placed over the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding reorders against hour of the day\n",
    "grouped_reorders = merged_reorders.groupby([\"order_hour_of_day\"])[\"reordered\"].aggregate(\"count\").reset_index()\n",
    "grouped_reorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "sns.barplot(grouped_reorders.order_hour_of_day, grouped_reorders.reordered)\n",
    "plt.ylabel('Total number of reordered products', fontsize=14)\n",
    "plt.xlabel('order_hour_of_day', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph shows that most products are reordered from 10-11AM followed by 1-3pm. This aligns with the number of products ordered during the week and the weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats_hour_of_day, p_value_hour_of_day = stats.shapiro(orders.order_hour_of_day)\n",
    "print(\"Test statistic for hour of the day is \",test_stats_hour_of_day)\n",
    "print(\"P-value for hour of the day is\",p_value_hour_of_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats_dow, p_value_dow =stats.shapiro(orders.order_dow)\n",
    "print(\"Test statistic for day of the week \",test_stats_dow)\n",
    "print(\"P-value for hour of the day is\",p_value_dow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stats_days_since_prior, p_value_days_since_prior =stats.shapiro(orders.days_since_prior_order)\n",
    "print(\"Test statistic for days_since_prior \",test_stats_days_since_prior)\n",
    "print(\"P-value for days_since_prior\",p_value_days_since_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.normaltest(orders.order_dow, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.normaltest(orders_new.days_since_prior_order,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.normaltest(orders.order_hour_of_day,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.normaltest(orders.order_number,axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.normaltest(order_products_prior.reordered,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a correlation plot to check for correlation between the different columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "corr_products = products.corr()\n",
    "sns.heatmap(corr_products, \n",
    "            xticklabels=corr_products.columns.values,\n",
    "            yticklabels=corr_products.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying the anderson-darling normality tests for the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson_results_dow = stats.anderson(orders.order_dow)\n",
    "print(anderson_results_dow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson_hour_of_day = stats.anderson(orders.order_hour_of_day)\n",
    "print(anderson_hour_of_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson_order_number = stats.anderson(orders.order_number)\n",
    "print(anderson_order_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson_days_since_prior_order = stats.anderson(orders_new.days_since_prior_order)\n",
    "print(anderson_days_since_prior_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson_reordered = stats.anderson(order_products_prior.reordered)\n",
    "print(anderson_reordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "corr_orders = orders.corr()\n",
    "sns.heatmap(corr_orders, \n",
    "            xticklabels=corr_orders.columns.values,\n",
    "            yticklabels=corr_orders.columns.values,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "corr_op = op_prior_merged.corr()\n",
    "sns.heatmap(corr_op, \n",
    "            xticklabels=corr_op.columns.values,\n",
    "            yticklabels=corr_op.columns.values, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "corr_reorders = merged_reorders.corr()\n",
    "sns.heatmap(corr_reorders, \n",
    "            xticklabels=corr_reorders.columns.values,\n",
    "            yticklabels=corr_reorders.columns.values, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_result_dow = stats.kstest(orders.order_dow, cdf='norm')\n",
    "ks_result_dow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_order_hour_of_day = stats.kstest(orders.order_hour_of_day, cdf='norm')\n",
    "ks_order_hour_of_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_order_number= stats.kstest(orders.order_number, cdf='norm')\n",
    "ks_order_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_days_since_prior_order = stats.kstest(orders_new.days_since_prior_order, cdf='norm')\n",
    "ks_days_since_prior_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_reordered = stats.kstest(order_products_prior.reordered, cdf='norm')\n",
    "ks_reordered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fetures that will be used to build our models are as follows:\n",
    "\n",
    "1) predict whether a product will be reordered or not :\n",
    "Order_id,Order_number,Average_days_between_orders,Nb_orders(Number of orders),Average_basket,Total items,Aisle,Department,Product,User_id,Order_hour_of_day,Order_dow(day of week),Days_since_prior_order,Days_since_ratio\n",
    "\n",
    "2) Predict which department a product will belong to :\n",
    "Order_id,Order_number,Average_days_between_orders,Nb_orders(Number of orders),Average_basket,Orders,Reorders,Reordered rate,Total items,User_id,Order_hour_of_day,Order_dow(day of week),Days_since_prior_order,Days_since_ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged1 = pd.merge(order_products_train, orders, on='order_id', how='left')\n",
    "merged1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged1 = pd.merge(merged1, products, on='product_id', how='left')\n",
    "df_merged1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging all the datasets to get a final train dataset\n",
    "df = pd.merge(df_merged1, departments, on='department_id', how='left')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.copy()\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['eval_set']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['add_to_cart_order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting average days between orders as a feature by using days_since_prior_order\n",
    "df['average_days_between_orders'] = orders_new.groupby('user_id')['days_since_prior_order'].mean().astype(np.float32)\n",
    "df['average_days_between_orders'] = df['average_days_between_orders'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_days_between_orders'] = df['average_days_between_orders'].replace(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of orders as a feature using the orders_new dataset\n",
    "df['nb_orders'] = orders_new.groupby('user_id').size().astype(np.int16)\n",
    "df['nb_orders'] = df['nb_orders'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the total items using the entire dataset \n",
    "df['total_items'] = df_merged1.groupby('user_id').size().astype(np.int16)\n",
    "df['total_items'] = df['total_items'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting average basket as afeature by using total items and number of orders\n",
    "df['average_basket'] = (df.total_items /df.nb_orders).astype(np.float32)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_basket'] = df['average_basket'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a days_since_ratio using days_since_prior_order and average_days_between_orders\n",
    "df['days_since_ratio'] = df.days_since_prior_order / df.average_days_between_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['user_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['product_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['department']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting hour, aisle, dept, product, days_since_prior_order, day of week into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hour = {c:i for i,c in enumerate(df['order_hour_of_day'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aisle = {c:i for i,c in enumerate(df['aisle_id'].unique())}\n",
    "dept = {c:i for i,c in enumerate(df['department_id'].unique())}\n",
    "product = {c:i for i,c in enumerate(df['product_id'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['aisle_new'] = [float(aisle[t]) for t in df.aisle_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dept_new'] = [float(dept[t]) for t in df.department_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['product_new'] = [float(product[t]) for t in df.product_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['order_hour_of_day_new'] = [float(hour[t]) for t in df.order_hour_of_day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['order_hour_of_day_new'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dow = {c:i for i,c in enumerate(df['order_dow'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['order_dow_new'] = [float(dow[t]) for t in df.order_dow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dspo = {c:i for i,c in enumerate(df['days_since_prior_order'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['days_since_prior_order__new'] = [float(dspo[t]) for t in df.days_since_prior_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reordered'] = df['reordered'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['aisle_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['department_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['order_hour_of_day']\n",
    "del df['order_dow']\n",
    "del df['days_since_prior_order']\n",
    "del df['product_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable to be predicted\n",
    "y=df['reordered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['reordered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final df which will be used to run our algorithms\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Xtest, ytr, ytest = train_test_split(df, y, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr=ytr.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest=ytest.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression model\n",
    "clf=(LogisticRegression(C=0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model\n",
    "clf.fit(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "pred=clf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score of Logistic Regression Model\n",
    "print(accuracy_score(clf.predict(Xtest), ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest model\n",
    "clfrf = RandomForestClassifier(max_features=\"log2\", max_depth=11, n_estimators=24,min_samples_split=1000, \n",
    "                               oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting\n",
    "clfrf.fit(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "predrf=clfrf.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score for the random forest model\n",
    "accuracy_score(predrf, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "feature_imp_reordered = pd.Series(clfrf.feature_importances_,index= df.columns)\n",
    "feature_imp_reordered.sort_values(ascending=False).plot(kind='Bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the feature_importances for predicting whether a product will be reoredered or not, the most important features turn out to be order_number, department, product, days since prior order and aisle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost Classifier\n",
    "clfa = AdaBoostClassifier( n_estimators=24,random_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting\n",
    "clfa.fit(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "preda = clfa.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy Score for AdaBoost Classifier\n",
    "accuracy_score(preda, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting Classifier\n",
    "clfg= GradientBoostingClassifier(max_features=\"log2\", max_depth=11, n_estimators=24,min_samples_split=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting\n",
    "clfg.fit(Xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions\n",
    "predg = clfg.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy score for Gradient Boosting Classifier\n",
    "accuracy_score(predg, ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for predicting the department variable\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating new features such as orders,reorders and reorder_rate for predicting the department variable\n",
    "\n",
    "df_new['orders'] = df_new.groupby(df_new.product_id).size().astype(np.int32) \n",
    "df_new['orders'] = df_new['orders'].replace(np.nan,0)\n",
    "df_new['reorders'] = df_new['reordered'].groupby(df_new.product_id).sum().astype(np.float32)\n",
    "df_new['reorders'] = df_new['reorders'].replace(np.nan,0)\n",
    "df_new['reorder_rate'] = (df_new.reorders / df_new.orders).astype(np.float32)\n",
    "df_new['reorder_rate'] = df_new['reorder_rate'].replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['average_days_between_orders'] = orders_new.groupby('user_id')['days_since_prior_order'].mean().astype(np.float32)\n",
    "df_new['average_days_between_orders'] = df_new['average_days_between_orders'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['average_days_between_orders'] = df_new['average_days_between_orders'].replace(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['nb_orders'] = orders_new.groupby('user_id').size().astype(np.int16)\n",
    "df_new['nb_orders'] = df_new['nb_orders'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['total_items'] = df_merged1.groupby('user_id').size().astype(np.int16)\n",
    "df_new['total_items'] = df_new['total_items'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['average_basket'] = (df_new.total_items /df_new.nb_orders).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['average_basket'] = df_new['average_basket'].replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['days_since_ratio'] = df_new.days_since_prior_order / df_new.average_days_between_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['order_hour_of_day_new'] = [float(hour[t]) for t in df_new.order_hour_of_day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['reordered'] = df_new['reordered'].astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['order_dow_new'] = [float(dow[t]) for t in df_new.order_dow]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['days_since_prior_order__new'] = [float(dspo[t]) for t in df_new.days_since_prior_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['dept_new'] = [float(dept[t]) for t in df_new.department_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['product_new'] = [float(product[t]) for t in df_new.product_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_new['days_since_prior_order']\n",
    "del df_new['order_dow']\n",
    "del df_new['order_hour_of_day']\n",
    "del df_new['department_id']\n",
    "del df_new['aisle_id']\n",
    "del df_new['product_id']\n",
    "#del df_new['user_id']\n",
    "del df_new['add_to_cart_order']\n",
    "del df_new['eval_set']\n",
    "del df_new['department']\n",
    "del df_new['product_name']\n",
    "del df_new['product_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final df which will be used to run our model to predict the category of department\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our variable to be predicted\n",
    "ynew = df_new['dept_new']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_new['dept_new']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrnew, Xtestnew, ytrnew, ytestnew = train_test_split(df_new, ynew, test_size=0.30, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest classifier\n",
    "clfrfnew = RandomForestClassifier(max_features=\"log2\", max_depth=11, n_estimators=24,min_samples_split=1000, \n",
    "                               oob_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting\n",
    "clfrfnew.fit(Xtrnew, ytrnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions and probabilities\n",
    "predrfnewp =clfrfnew.predict_proba(Xtestnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "feature_imp_dept = pd.Series(clfrfnew.feature_importances_,index= df_new.columns)\n",
    "feature_imp_dept.sort_values(ascending=False).plot(kind='Bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important features while predicting the department are: Reordered, day of week, order number, user_id and order_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log loss for the Random Forest model\n",
    "log_loss( ytestnew,predrfnewp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting Classifier\n",
    "clfgb= GradientBoostingClassifier(max_features=\"log2\", max_depth=11, n_estimators=24,min_samples_split=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting\n",
    "clfgb.fit(Xtrnew,ytrnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions and probabilties\n",
    "predgbnewp =clfgb.predict_proba(Xtestnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log loss score for Gradient Boosting Classifier\n",
    "log_loss( ytestnew,predgbnewp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost Classifier\n",
    "clfada = AdaBoostClassifier( n_estimators=24,random_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting\n",
    "clfada.fit(Xtrnew, ytrnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions and probabilities\n",
    "predadap = clfada.predict_proba(Xtestnew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log loss score for AdaBoost Classifier\n",
    "log_loss( ytestnew,predadap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model for predicting whether a product will be reorodered or not is the Gradient Boosting Classifier with 0.67 accuracy. Whereas while predicting the category of the department, Random Forest Classifier is the best model with a log loss score of 2.342."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
